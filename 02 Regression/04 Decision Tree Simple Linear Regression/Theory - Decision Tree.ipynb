{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Decision Tree\n",
    "\n",
    "### Types of Decision Trees\n",
    "1. Classification Trees\n",
    "2. Regression Trees"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classification Trees\n",
    "\n",
    "In the case of qualitative data or categorical data, we use classification trees. in classification, we classify using classification error rate, Gini impurity and entropy.\n",
    "Let’s understand these terms in detail.\n",
    "\n",
    "The root node for classification is selected using the Info Gain or Gini Index.\n",
    "#### 1. Methods the Calculate the Randomness in the subset.\n",
    "    1.1 Entropy + Information Gain (Works only with Numerical Features)\n",
    "    2.2. Ginni Impurity + Gini Indexing (Works with both Numerical and Categorical features)\n",
    "\n",
    "####  After Calculating the Impurity we Check For the Information Gain\n",
    "\n",
    "#### 1.1. Entropy\n",
    "\n",
    "Entropy is the measure of Impurity / randomness in the data.\n",
    "\n",
    "When we split our nodes into two regions and put different observations in both the regions, the main goal is to reduce the entropy and divide our data cleanly. \n",
    "\n",
    "* A region is 'clean' (low entropy) when it contains data with the same labels \n",
    "* and 'Random' (high entropy) if there is a mixture of labels present.\n",
    "\n",
    "Let’s suppose there are total ‘T’ observations and we need to classify them into categories 'Yes' and 'No'.\n",
    "\n",
    "Let’s say that category 'Yes' has ‘y’ observations and category 'No' has ‘T-y = n’ observations.\n",
    "\n",
    "    p = y/T  and    q = n / T \n",
    "\n",
    "then, entropy for the given set is:\n",
    "\n",
    "$$ E = -p \\times log_2(p) – q \\times log_2(q) $$ \n",
    "\n",
    "####  Information Gain\n",
    "\n",
    "Information gain calculates the decrease in entropy after splitting a node. It is the difference between entropies before and after the split. The more the information gain, the more entropy is removed.\n",
    "\n",
    "And Information Gain is Given as:\n",
    "$$ Info Gain(S,Sv) = E(S) - \\sum \\frac{|Sv|}{|S|} \\times H(Sv)$$\n",
    "\n",
    "        Info Gain(S,Sv) = E(S) - Weighted sum of Entropies of Branches\n",
    "\n",
    "S - Parent Node..... |S| = No of Observations in the parent Node.......E(s) - Entropy of the Parent Node\n",
    "\n",
    "Sv - Child Node......|Sv| = No of Observations in the Child Node......E(Sv) - Entropy of the Child Node."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"entropy1.PNG\" width=\"500\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2. Ginni Impurity\n",
    "\n",
    "Ginni Impurity Gives the Measure of Impurity in our data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$ Ginni Impurity = 1- \\sum \\left(p_i^2\\right )  $$\n",
    "$$i.e. Ginni Impurity = 1- \\left(p^2 + q^2 \\right )  $$\n",
    "\n",
    "**Ginni Impurity is computationally light compared to Entropy, And Hence Faster Than Entropy.**\n",
    "\n",
    "### Gini Index\n",
    "$$ Gini Index = Weighted Sum of Gini Impurities =  \\sum \\frac{|N_{Category}|}{|N_{Feature}|} \\times Gini Impurity(Category)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Maths behind Decision Tree Classifier\n",
    "We will use a simple dataset which contains information about students of different classes and gender and see whether they stay in school's hostel or not."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src='data_class.PNG' width=\"250\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's try and understand how the root node is selected by calcualting **gini impurity**.\n",
    "\n",
    "We have two features which we can use for nodes: \"Class\" and \"Gender\".\n",
    "\n",
    "We will calculate Gini Index for each of the features and then select that feature which has least Gini Index.\n",
    "\n",
    "Let's review the formula for calculating Gini Impurity:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src='example/gini.PNG' width=\"200\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's start with class, we will try to calculate gini impurity for all different Categories in \"class\" feature. \n",
    "\n",
    "<img src='example/1.PNG' width=\"500\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src='example/2.PNG' width=\"500\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gini Impurity(class = 8) 0.4444444444444445\n",
      "Gini Impurity(class = 9) 0.4444444444444445\n",
      "Gini Impurity(class = 10) 0.375\n",
      "Gini Impurity(class = 11) 0.375\n"
     ]
    }
   ],
   "source": [
    "p, q = 2/3, 1/3\n",
    "print(\"Gini Impurity(class = 8)\",1 - p**2 - q**2)\n",
    "\n",
    "p, q = 2/3, 1/3\n",
    "print(\"Gini Impurity(class = 9)\",1 - p**2 - q**2)\n",
    "\n",
    "p, q = 1/4, 3/4\n",
    "print(\"Gini Impurity(class = 10)\",1 - p**2 - q**2)\n",
    "\n",
    "p, q = 3/4, 1/4\n",
    "print(\"Gini Impurity(class = 11)\",1 - p**2 - q**2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<img src='example/3.1.PNG' width=\"500\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now Let's Calculate the Gini Index for 'class' feature.\n",
    "$$ Gini Index = Weighted Sum of Gini Impurities =  \\sum \\frac{|N_{Category}|}{|N_{Feature}|} \\times Gini Impurity(Category)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<img src='example/3.PNG' width=\"500\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<img src='example/4.PNG' width=\"500\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.40476190476190477"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# weighted sum for feature 'Class'\n",
    "((3/14)*(4/9))*2 + ((4/14)*(6/16))*2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Repeating the same process for feature Gender:\n",
    "\n",
    "<img src='example/5.PNG' width=\"500\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src='example/6.PNG' width=\"500\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.46875"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(1-(5/8)**2-(3/8)**2)\n",
    "\n",
    "(1-(1/2)**2-(1/2)**2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<img src='example/7.PNG' width=\"500\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<img src='example/8.PNG' width=\"500\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "This is how our Decision tree node is selected by calculating gini impurity for each node individually.\n",
    "If the number of feautures increases, then we just need to repeat the same steps after the selection of the root node."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will try and find the root nodes for the same dataset by calculating entropy and information gain.\n",
    "\n",
    "DataSet:\n",
    "\n",
    "<img src='data_class.PNG' width=\"200\">\n",
    "\n",
    "We have two features and we will try to choose the root node by calculating the information gain by splitting each feature, and for that we first need to calculate the entropy for each of the categories in each feature."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$ Information Gain (Feature) = Entropy Of the Raw Data set - Information from (Feature)$$\n",
    "\n",
    "$$ Information Gain (Feature) = E(S) - I(Feature)  $$\n",
    "   \n",
    "$ Where :$\n",
    "$$ E(S) = Entropy Of The Target Column = -p \\times log_2(p) - q\\times log_2(q) $$\n",
    "\n",
    "$$ I(feature) = Information From Feature = Weighted Sum of Entropies of Categories In The Feature $$ \n",
    "    \n",
    "$$ Weighted sum of Entropies of Categories = \\sum \\frac{|N_{Category}|}{|N_{Feature}|} \\times Entropy (Category)  $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's start with feature \"class\" :\n",
    "\n",
    "<img src='example/9.PNG' width=\"500\">\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Categories in the feature \"Class\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src='example/10.1.PNG' width=\"500\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calculating entropies for Each Category "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<img src='example/11.PNG' width=\"500\">\n",
    "\n",
    "<img src='example/12.PNG' width=\"500\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now Calculating **Information Gain** for the feature \"Class\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<img src='example/13.PNG' width=\"500\">\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let' see the information gain from feature \"gender\" :\n",
    "\n",
    "<img src='example/10.2.PNG' width=\"500\">\n",
    "\n",
    "<img src='example/14.PNG' width=\"500\">\n",
    "\n",
    "<img src='example/15.PNG' width=\"500\">\n",
    "\n",
    "<img src='example/16.PNG' width=\"500\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now lets talk about the Continuous Numerical features.\n",
    "\n",
    "In the case of Continuous Numerical Features we will use \"Gini Indexing\" method only.\n",
    "\n",
    "And in order to split the data we need a threshold value around which we can split the data. This threshold value could be Means, Median or Mode(for categorical feature).\n",
    "\n",
    "In the Example below we are using a random data and threshold value is mean of the data. so the feature column will be split as : \n",
    "1. Values in Feature  <= Mean of Feature\n",
    "* Values in Feature  > Mean of Feature"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src='example/1_1.jpg' width=\"500\">\n",
    "<img src='example/1_2.jpg' width=\"500\">\n",
    "<img src='example/1_3.jpg' width=\"500\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note : As It's a Random Data set the Values are accidentally similar hence I've Intentionally made the valye for Gini Index(D) smaller to show you further steps.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As G.I.(D) is the smallest , Feature \"D\" is selected as the First Root node for the tree.\n",
    "\n",
    "And this process will be further repeated till we get the leafe nodes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Tree Pruning\n",
    "Tree pruning is the method of trimming down a full tree to reduce the complexity and variance in the data. Just as we regularised linear regression, we can also regularise the decision tree model by adding a new term. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Post-pruning\n",
    "\n",
    "Post-pruning, also known as backward pruning, is the process where the decision tree is generated first and then the non-significant branches are removed. Cross-validation set of data is used to check the effect of pruning and tests whether expanding a node will make an improvement or not. If any improvement is there then we continue by expanding that node else if there is reduction in accuracy then the node not be expanded and should be converted in a leaf node.\n",
    "\n",
    "\n",
    "#### Pre-pruning\n",
    "\n",
    "Pre-pruning, also known as forward pruning, stops the non-significant branches from generating. It uses a condition to decide when should it terminate splitting of some of the branches prematurely as the tree is generated.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Regression Tree - YouTube Video\n",
    "\n",
    "https://www.youtube.com/watch?v=g9c66TUylZ4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Different Algorithms for Decision Tree\n",
    "\n",
    "* ID3 (Iterative Dichotomiser) : It is one of the algorithms used to construct decision tree for classification. It uses Information gain as the criteria for finding the root nodes and splitting them. It only accepts categorical attributes.\n",
    "\n",
    "* C4.5 : It is an extension of ID3 algorithm, and better than ID3 as it deals both continuous and discreet values. It is also used for classfication purposes.\n",
    "\n",
    "* Classfication and Regression Tree (CART) : It is the most popular algorithm used for constructing decison trees. It uses ginni impurity as the default calculation for selecting root nodes, however one can use \"entropy\" for criteria as well. This algorithm works on both regression as well as classfication problems. We will use this algorithm in our pyhton implementation. \n",
    "\n",
    "\n",
    "Entropy and Ginni impurity can be used reversibly. It doesn't affects the result much. Although, ginni is easier to compute than entropy, since entropy has a log term calculation. That's why CART algorithm uses ginni as the default algorithm.\n",
    "\n",
    "If we plot ginni vs entropy graph, we can see there is not much difference between them:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Advantages of Decision Tree:\n",
    "\n",
    "   * It can be used for both Regression and Classification problems.\n",
    "   * Decision Trees are very easy to grasp as the rules of splitting is clearly mentioned.\n",
    "   * Scaling and normalization are not needed.\n",
    "\n",
    "##### Disadvantages of Decision Tree:\n",
    "\n",
    "   * A small change in data can cause instability in the model because of the greedy approach.\n",
    "   * Probability of overfitting is very high for Decision Trees.\n",
    "   * It takes more time to train a decision tree model than other classification algorithms."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cross-Validation tackles Overfitting\n",
    "\n",
    "Suppose you train a model on a given dataset using any specific algorithm. You tried to find the accuracy of the trained model using the same training data and found the accuracy to be 95% or maybe even 100%. \n",
    "\n",
    "\n",
    "What does this mean? Is your model ready for prediction? The answer is no.\n",
    "\n",
    "\n",
    "Why? Because your model has trained itself on the given data, i.e. it knows the data and it has generalized over it very well. But when you try and predict over a new set of data, it’s most likely to give you very bad accuracy, because it has never seen the data before and thus it fails to generalizes well over it. **This is the problem of overfitting.**\n",
    "\n",
    "\n",
    "To tackle such problem, Cross-validation comes into the picture. Cross-validation is a resampling technique with a basic idea of dividing the training dataset into two parts i.e. train and test. On one part(train) you try to train the model and on the second part(test) i.e. the data which is unseen for the model, you make the prediction and check how well your model works on it. If the model works with good accuracy on your test data, it means that the model has not overfitted the training data and can be trusted with the prediction, whereas if it performs with bad accuracy then our model is not to be trusted and we need to tweak our algorithm."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let’s see the different approaches of Cross-Validation:\n",
    "\n",
    "#### 1. Hold Out Method: \n",
    "\n",
    "It is the most basic of the CV techniques. It simply divides the dataset into two sets of training and test. The training dataset is used to train the model and then test data is fitted in the trained model to make predictions. We check the accuracy and assess our model on that basis. This method is used as it is computationally less costly. But the evaluation based on the Hold-out set can have a high variance because it depends heavily on which data points end up in the training set and which in test data. The evaluation will be different every time this division changes.\n",
    "\n",
    "#### 2. k-fold Cross-Validation (To tackle high variance of Hold Out)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<img src=\"cv1.png\" width=\"\"> \n",
    "\n",
    " img_src:Wikipedia\n",
    "\n",
    "To tackle the high variance of Hold-out method, the k-fold method is used. The idea is simple, divide the whole dataset into ‘k’ sets preferably of equal sizes. Then the first set is selected as the test set and the rest ‘k-1’ sets are used to train the data. Error is calculated for this particular dataset.\n",
    "\n",
    "\n",
    "Then the steps are repeated, i.e. the second set is selected as the test data, and the remaining ‘k-1’ sets are used as the training data. Again, the error is calculated. Similarly, the process continues for ‘k’ times. In the end, the CV error is given as the mean of the total errors calculated individually, mathematically given as:\n",
    "\n",
    "<img src=\"cv2.png\" width=\"\"> \n",
    "                "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The variance in error decreases with the increase in ‘k’. \n",
    "\n",
    "The **disadvantage of k-fold cv is that it is computationally expensive** as the algorithm runs from scratch for ‘k’ times.\n",
    "\n",
    "#### 2.1. Leave One Out Cross Validation (LOOCV)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"cv3.png\" width=\"400\"> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " **LOOCV is a special case of k-fold CV**, where k becomes equal to n (number of observations). So instead of creating two subsets, it selects a single observation as a test data and rest of data as the training data. The error is calculated for this test observations. Now, the second observation is selected as test data, and the rest of the data is used as the training set. Again, the error is calculated for this particular test observation. This process continues ‘n’ times and in the end, CV error is calculated as:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<img src=\"cv4.png\" width=\"\"> \n",
    "                                             \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bias Variance tradeoff for k-fold CV, LOOCV and Holdout Set CV\n",
    "\n",
    "There is a very good explanation given in the ISLR Book as given below:\n",
    "\n",
    "\n",
    "A k-fold CV with k < n has a computational advantage to LOOCV. But putting computational issues aside,\n",
    "\n",
    "a less obvious but potentially more important advantage of k-fold CV is that it often gives more accurate estimates of the test error rate than does LOOCV.\n",
    "The validation set approach can lead to over-estimates of the test error rate since in this approach the training set used to fit the statistical learning method contains only half the observations of the entire data set. Using this logic, it is not hard to see that LOOCV will give approximately unbiased estimates of the test error since each training set contains n − 1 observations, which is almost as many as the number of observations in the full data set. And performing k-fold CV for, say, k = 5 or k = 10 will lead to an intermediate level of bias since each training set contains (k − 1)n/k observations—fewer than\n",
    "in the LOOCV approach, but substantially more than in the validation set approach. \n",
    "\n",
    "Therefore, from the perspective of bias reduction, it is clear that LOOCV is to be preferred to k-fold CV. However, we know that bias is not the only source for concern in an estimating procedure; we must also consider the procedure’s variance. It turns out that LOOCV has higher variance than does k-fold CV with k < n. \n",
    "\n",
    "Why is this the case? When we perform LOOCV, we are in effect averaging the outputs of n fitted models, each of which is trained on an almost identical set of observations; therefore, these outputs are highly (positively) correlated with each other. In contrast, when we perform k-fold CV with k < n, we are averaging the outputs of k fitted models that are somewhat less correlated with each other since the overlap between the training sets in each model is smaller. Since the mean of many highly correlated quantities has higher variance than does the mean of many quantities that are not as highly correlated, the test error estimate resulting from LOOCV tends to have higher variance than does the test error estimate resulting from k-fold CV."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
