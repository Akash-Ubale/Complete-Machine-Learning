{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logistic Regression\n",
    "\n",
    "It calculates the probability that a given value belongs to a specific class. If the probability is more than 50%, it assigns the value in that particular class else if the probability is less than 50%, the value is assigned to the other class. Therefore, we can say that logistic regression acts as a binary classifier.\n",
    "\n",
    "##### Working of a Logistic Model\n",
    "For linear regression, the model is defined by:\n",
    "$y = \\beta_0 + \\beta_1 x  $       ...............(i)\n",
    "\n",
    "and for logistic regression, we calculate probability, i.e. y is the probability of a given variable x belonging to a certain class. Thus, the value of y should lie between 0 and 1.\n",
    "\n",
    "But, when we use equation (i) to calculate probability, we would get values less than 0 as well as greater than 1."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Sigmoid function \n",
    "\n",
    "We use the sigmoid function as the underlying function in Logistic regression. Mathematically and graphically, it is shown as:\n",
    "\n",
    "$$ Sigmoid = \\frac{1}{1+e^{-t}} $$\n",
    "\n",
    "<img src=\"sigmoid.PNG\" width=\"400\">\n",
    "\n",
    "**Why do we use the Sigmoid Function?**\n",
    "\n",
    "1.\tThe sigmoid function’s range is between 0 and 1. Thus it’s useful in calculating the probability for the  Logistic function.\n",
    "*\t It’s derivative is easy to calculate than other functions which is useful during gradient descent calculation.\n",
    "*\tIt is a simple way of introducing non-linearity to the model.\n",
    "\n",
    "Although there are other functions as well, which can be used, but sigmoid is the most common function used for logistic regression.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The logistic function is given as:\n",
    "\n",
    "<img src=\"logistic_function.PNG\" width=\"250\">\n",
    "\n",
    "We can see that the logit function is linear in terms with x.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multiple Logistic Function\n",
    "\n",
    "We can generalise the simple logistic function for multiple features as:\n",
    "<img src=\"multi.PNG\" width=\"300\">\n",
    "\n",
    "And the logit function can be written as:\n",
    "\n",
    "<img src=\"logit.PNG\" width=\"400\">\n",
    "\n",
    "The coefficients are calculated the same we did for simple logistic function, by passing the above equation in the cost function.\n",
    "\n",
    "Just like we did in multilinear regression, we will check for correlation between different features for Multi logistic as well."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multinomial Logistics Regression( Number of Labels >2)\n",
    "\n",
    "we have libraries that we can use to perform multinomial logistic regression, **we rarely use logistic regression for classification problems where the number of classes is more than 2.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Learning Algorithm\n",
    "\n",
    "we will use gradient descent instead. Specifically we will use batch gradient descent which calculates the gradient from all data points in the data set.\n",
    "\n",
    "Luckily, our \"cross-entropy\" error measure is convex so there is only one minimum. Thus the minimum we arrive at is the global minimum."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation of a Classification Model\n",
    "\n",
    "For a  regression problem, we have different metrics like R Squared score, Mean Squared Error etc. what are the metrics to measure the credibility of a classification model?\n",
    "\n",
    "#### Metrics\n",
    "\n",
    "In a regression problem, the accuracy is generally measured in terms of the difference in the actual values and the predicted values.\n",
    "\n",
    "In a classification problem, the credibility of the model is measured using the confusion matrix generated, i.e., how accurately the true positives and true negatives were predicted.\n",
    "\n",
    "The different metrics used for this purpose are:\n",
    "1. Accuracy\n",
    "- Recall\n",
    "- Precision\n",
    "- F1 Score\n",
    "- Specifity\n",
    "- AUC( Area Under the Curve)\n",
    "- ROC(Receiver Operator Characteristic)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Confusion Matrix\n",
    "\n",
    "A typical confusion matrix looks like the figure shown.\n",
    "\n",
    "<img src=\"confusionMatrix.PNG\" width=\"300\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Accuracy : Correct predictions out of total predictions.\n",
    "\n",
    "The mathematical formula is :\n",
    "\n",
    "  $$ Accuracy=  \\frac{ (TP+TN)}{(TP+TN+FP+FN)} $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Recall or Sensitivity : True Positives out of Total Predicted Positives\n",
    "The mathematical formula is:\n",
    "\n",
    "$$ Recall=  \\frac{TP}{(TP+FN)} $$\n",
    "\n",
    "Sensitivity is a measure of : from the total number of (Actual)positive results how many positives were correctly predicted by the model.\n",
    "\n",
    "Let’s suppose in the previous model, the model gave 50 correct predictions(TP) but failed to identify 200 cancer patients(FN). Recall in that case will be:\n",
    "\n",
    "Recall=$ \\frac {50}{(50+200)} $= 0.2 (The model was able to recall only 20% of the cancer patients)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Precision : True Positives out of Actual Positives.\n",
    "\n",
    "Precision is a measure of amongst all the positive predictions, how many of them were actually positive. Mathematically,\n",
    "\n",
    "Precision=$ \\frac {TP}{(TP+FP)} $\n",
    "\n",
    "Let’s suppose in the previous example, the model identified 50 people as cancer patients(TP) but also raised a  false alarm for 100 patients(FP). Hence,\n",
    "\n",
    "Precision=$ \\frac {50}{(50+100)} $=0.33 (The model only has a precision of 33%)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### But we have a problem!!\n",
    "\n",
    "As evident from the previous example, the model had a very high Accuracy but performed poorly in terms of Precision and Recall. So, necessarily _Accuracy_ is not the metric to use for evaluating the model in this case.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A Trade-off?\n",
    "\n",
    "With an increase in the Recall, there is a drop in Precision of the model.\n",
    "\n",
    "So the question is - what to go for? Precision or Recall?\n",
    "\n",
    "Well, the answer is: it depends on the business requirement.\n",
    "\n",
    "For example, if you are predicting cancer, you need a 100 % recall. But suppose you are predicting whether a person is innocent or not, you need 100% precision.\n",
    "\n",
    "Can we maximise both at the same time? No\n",
    "\n",
    "So, is there  a need for a better metric then?\n",
    "\n",
    "Yes. And it’s called an _F1 Score_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. F1 Score\n",
    "\n",
    "F1 score is defined as the harmonic mean of Precision and Recall. \n",
    "\n",
    "The mathematical formula is:\n",
    "        $$ F1 Score=  2 \\times \\left( \\frac { Precision \\times Recall}{Precision+Recall} \\right)$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Specificity or True Negative Rate\n",
    "\n",
    "True Negatives out of the predictions.\n",
    "\n",
    "  $$ Specificity = \\frac {TN}{(TN+FP)} $$\n",
    "\n",
    "Similarly, False Positive rate can be defined as:  (1- specificity)\n",
    "Or,  $ \\frac {FP}{(TN+FP)} $\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
