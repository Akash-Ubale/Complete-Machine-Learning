{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Notes_Random_Forest"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random Forest is a Bagging Algorithm which uses Decision Trees as its base model.\n",
    "\n",
    "### What is Bagging?\n",
    "\n",
    "Bagging is an ensembled technique in which \n",
    "1. We Sample the data **with replacement**,  (The data sampling is called Boot-strap.)\n",
    "\n",
    "2. We feed that data to Multiple base models.\n",
    "\n",
    "3. And finally we aggregate the predictions from these models.\n",
    "    1. For Classification we go for voting\n",
    "    2. for Regression we go for Mean or Median of the results.\n",
    "    \n",
    "### What are the Advantages of Bagging over Single Model?\n",
    "1. Bagging significantly decreases the variance without increasing bias.\n",
    "\n",
    "2. Bagging is Stable in case of outliers, Because the outliers gets randomly separated in the sampled datasets.\n",
    "\n",
    "3. The Bagging Method works well bcz of Diversity in the Training data, which is caused by the Boot-strap sampling.\n",
    "\n",
    "4. Bagging can save Computational time and resources by traing the model on reltively small subsets of data and still increase the accuracy.\n",
    "\n",
    "### What are the Disadvantage of a Bagging Model? \n",
    "\n",
    "The main disadvantage of Bagging is that it improves the accuracy of the model on the expense of interpretability\n",
    "\n",
    "### What is Out-Of-Bag Evaluation?\n",
    "\n",
    "In Bagging the sampled datasets contain only a fraction of the main data. There are always some data points which never gets used for training the model(these instances are called out of bag instances) and hence can be used as the validation dataset. Hence we do not need a separate validation dataset.\n",
    "\n",
    "## Python implementation of Bagging."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_breast_cancer\n",
    "dataset = load_breast_cancer()\n",
    "\n",
    "# Separating Features and Columns\n",
    "X = dataset.data\n",
    "y = dataset.target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Splitting the Training and testing data\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training an Individual Algorithm without Bagging."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KNN Score:  0.9385964912280702\n",
      "\n",
      "Confusion Mtrix for KNN:\n",
      " [[44  3]\n",
      " [ 4 63]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score\n",
    "knn = KNeighborsClassifier(n_neighbors=5)\n",
    "knn.fit(X_train, y_train)\n",
    "\n",
    "y_pred = knn.predict(X_test)\n",
    "print(\"KNN Score: \",knn.score(X_test, y_test))\n",
    "print(\"\\nConfusion Mtrix for KNN:\\n\", confusion_matrix(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Now let's use bagging over KNN "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bagging Score:  0.9649122807017544\n",
      "\n",
      "Confusioin Matrix:\n",
      " [[44  3]\n",
      " [ 1 66]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import BaggingClassifier\n",
    "knn2 = KNeighborsClassifier(n_neighbors=5)\n",
    "bag_knn = BaggingClassifier(base_estimator=knn2,\n",
    "                            n_estimators=10,\n",
    "                            max_samples=0.5,\n",
    "                            oob_score=True,\n",
    "                            random_state=3 )\n",
    "bag_knn.fit(X_train, y_train)\n",
    "y_pred2 = bag_knn.predict(X_test)\n",
    "print(\"Bagging Score: \", bag_knn.score(X_test, y_test))\n",
    "print(\"\\nConfusioin Matrix:\\n\", confusion_matrix(y_test, y_pred2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Great the Accuracy Improved by 3%.\n",
    "\n",
    "### Now Let's Talk about Random Forest\n",
    "\n",
    "## What is Random Forest?\n",
    "\n",
    "Decision tree has low bias and high variance hence Decision Trees tends to overfit the dataset. Hence we can use the Decision trees as base model in Bagging so that the variance gets reduced.\n",
    "\n",
    "Or Instead of using bagging model with decision trees we can use **Random Forest**.\n",
    "\n",
    "### What's the advantage of Random Forest over Bagging with decision trees?\n",
    "The advantage of Random forests over bagging models is that the Random Forests makes a tweak in the working algorithm of bagging model to decrease the correlation in trees. The idea is to introduce more randomness while creating trees which will help in reducing correlation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Explain the Random Forest Algorithm.\n",
    "\n",
    "1. just like Bagging Algorithm we use boot-strap for Sampling the dataset.\n",
    "2. Then we train our tree models on this data subsets and we allow the trees to grow to depths.\n",
    "    \n",
    "    Now The main difference between the Bagging with Decision Trees and Random Forest Algorithm is How the Trees are formed in Random Forest.\n",
    "    \n",
    "    In Bagging we allow all the Features/ Columns/ Predictors to be used for Splitting the node.\n",
    "    But in Random Forest each time a split is to happen a random sample of 'm' predictors /features are selected out of total predictors for sampling, i.e. Column sampling with replacement is introduced at the time of Splitting.\n",
    "    \n",
    "### Why The Column Sampling with replacement is done at the time of tree splitting?\n",
    "    \n",
    "Suppose in those total ‘p’ predictors, 1 predictor is very strong. Now in each sample this predictor will remain the strongest. So, whenever trees will be built for these sampled data, this predictor will be chosen by all the trees for splitting and thus will result in similar kind of tree formation for each bootstrap model. This introduces correaltion in the dataset and averaging correalted dataset results do not lead low variance. That’s why in random forest the choice for selecting node for split is limited and it introduces randomness in the formation of the trees as well.\n",
    "    \n",
    "    Most of the predictors are not allowed to be considered for split.\n",
    "    Generally, value of ‘m’ is taken as m ≈ √p , where ‘p’ is the number of predictors in the sample.\n",
    "\n",
    "    When m=p , the random forest model becomes bagging model.   \n",
    "              \n",
    "    *This method is also referred as “Feature Sampling”\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What are the Advantages of Random Forest?\n",
    "1. It can be used for both Classification and Regression.\n",
    "2. Since the base model is a Decision Tree, missing values gets handled.\n",
    "3. Outliers doesn't affect the model much.\n",
    "4. As the variance is very low the results are very accurate.\n",
    "\n",
    "\n",
    "### What are the Disadvantages of Random Forest?\n",
    "1. High coimputational time.\n",
    "2. Results are very hard to interpret.\n",
    "\n",
    "# Let's see the Python Implementation of Random Forest.\n",
    "\n",
    "Here we are classifying the wine depending on it's quality."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\akash\\.conda\\envs\\General\\lib\\importlib\\_bootstrap.py:219: RuntimeWarning: numpy.ufunc size changed, may indicate binary incompatibility. Expected 192 from C header, got 216 from PyObject\n",
      "  return f(*args, **kwds)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>fixed acidity</th>\n",
       "      <th>volatile acidity</th>\n",
       "      <th>citric acid</th>\n",
       "      <th>residual sugar</th>\n",
       "      <th>chlorides</th>\n",
       "      <th>free sulfur dioxide</th>\n",
       "      <th>total sulfur dioxide</th>\n",
       "      <th>density</th>\n",
       "      <th>pH</th>\n",
       "      <th>sulphates</th>\n",
       "      <th>alcohol</th>\n",
       "      <th>quality</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>7.4</td>\n",
       "      <td>0.70</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1.9</td>\n",
       "      <td>0.076</td>\n",
       "      <td>11.0</td>\n",
       "      <td>34.0</td>\n",
       "      <td>0.9978</td>\n",
       "      <td>3.51</td>\n",
       "      <td>0.56</td>\n",
       "      <td>9.4</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>7.8</td>\n",
       "      <td>0.88</td>\n",
       "      <td>0.00</td>\n",
       "      <td>2.6</td>\n",
       "      <td>0.098</td>\n",
       "      <td>25.0</td>\n",
       "      <td>67.0</td>\n",
       "      <td>0.9968</td>\n",
       "      <td>3.20</td>\n",
       "      <td>0.68</td>\n",
       "      <td>9.8</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>7.8</td>\n",
       "      <td>0.76</td>\n",
       "      <td>0.04</td>\n",
       "      <td>2.3</td>\n",
       "      <td>0.092</td>\n",
       "      <td>15.0</td>\n",
       "      <td>54.0</td>\n",
       "      <td>0.9970</td>\n",
       "      <td>3.26</td>\n",
       "      <td>0.65</td>\n",
       "      <td>9.8</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>11.2</td>\n",
       "      <td>0.28</td>\n",
       "      <td>0.56</td>\n",
       "      <td>1.9</td>\n",
       "      <td>0.075</td>\n",
       "      <td>17.0</td>\n",
       "      <td>60.0</td>\n",
       "      <td>0.9980</td>\n",
       "      <td>3.16</td>\n",
       "      <td>0.58</td>\n",
       "      <td>9.8</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>7.4</td>\n",
       "      <td>0.70</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1.9</td>\n",
       "      <td>0.076</td>\n",
       "      <td>11.0</td>\n",
       "      <td>34.0</td>\n",
       "      <td>0.9978</td>\n",
       "      <td>3.51</td>\n",
       "      <td>0.56</td>\n",
       "      <td>9.4</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   fixed acidity  volatile acidity  citric acid  residual sugar  chlorides  \\\n",
       "0            7.4              0.70         0.00             1.9      0.076   \n",
       "1            7.8              0.88         0.00             2.6      0.098   \n",
       "2            7.8              0.76         0.04             2.3      0.092   \n",
       "3           11.2              0.28         0.56             1.9      0.075   \n",
       "4            7.4              0.70         0.00             1.9      0.076   \n",
       "\n",
       "   free sulfur dioxide  total sulfur dioxide  density    pH  sulphates  \\\n",
       "0                 11.0                  34.0   0.9978  3.51       0.56   \n",
       "1                 25.0                  67.0   0.9968  3.20       0.68   \n",
       "2                 15.0                  54.0   0.9970  3.26       0.65   \n",
       "3                 17.0                  60.0   0.9980  3.16       0.58   \n",
       "4                 11.0                  34.0   0.9978  3.51       0.56   \n",
       "\n",
       "   alcohol  quality  \n",
       "0      9.4        5  \n",
       "1      9.8        5  \n",
       "2      9.8        5  \n",
       "3      9.8        6  \n",
       "4      9.4        5  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd \n",
    "dataset = pd.read_csv(\"winequality_red-Copy1.csv\")\n",
    "dataset.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = dataset.iloc[:, :-1].values\n",
    "y = dataset.iloc[:, -1].values\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### First let's check the results of Individual Decisipon Tree Model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of a single Decision Tree model: 0.653125\n",
      "\n",
      "Confusion Matrix:\n",
      " [[ 0  0  0  2  0  0]\n",
      " [ 1  2  4  4  0  0]\n",
      " [ 2  4 96 31  2  0]\n",
      " [ 1  3 31 94 12  1]\n",
      " [ 0  0  1  5 17  4]\n",
      " [ 0  0  0  2  1  0]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score\n",
    "dt_classifier = DecisionTreeClassifier()\n",
    "\n",
    "dt_classifier.fit(X_train, y_train)\n",
    "\n",
    "y_pred = dt_classifier.predict(X_test)\n",
    "\n",
    "print(\"Accuracy of a single Decision Tree model:\", accuracy_score(y_test, y_pred))\n",
    "print(\"\\nConfusion Matrix:\\n\", confusion_matrix(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Now let's try the Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of Random Forest: 0.7125\n",
      "\n",
      "Confusion matrix for Random Forest:\n",
      " [[ 0  0  0  2  0  0]\n",
      " [ 1  2  4  4  0  0]\n",
      " [ 2  4 96 31  2  0]\n",
      " [ 1  3 31 94 12  1]\n",
      " [ 0  0  1  5 17  4]\n",
      " [ 0  0  0  2  1  0]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "rf_classifier = RandomForestClassifier(random_state=0)\n",
    "\n",
    "rf_classifier.fit(X_train, y_train)\n",
    "\n",
    "y_pred2 = rf_classifier.predict(X_test)\n",
    "\n",
    "print(\"Accuracy of Random Forest:\", accuracy_score(y_test, y_pred2))\n",
    "\n",
    "print(\"\\nConfusion matrix for Random Forest:\\n\", confusion_matrix(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Our accuracy Improved by 6%."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
