{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Notes Artificial Neural Network\n",
    "\n",
    "# The Neuron\n",
    "\n",
    "## Physical Neuron Looks like this:\n",
    "<img src=\"Physical_Neuron.png\" width=500>\n",
    "\n",
    "\n",
    "## Artificial Neuron:\n",
    "<img src=\"Artificial_Neuron.png\" width=750>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Types of Activation Functions\n",
    "### 1. Threshold Function\n",
    "### $$ \\phi(X) = \\binom{1~if ~X >= 0}{0 ~if ~X<0}$$\n",
    "\n",
    "### 2. Sigmoid Function (Smooth Function)\n",
    "### $$ \\phi(X) = \\frac{1}{1+e^{-X}} $$ \n",
    "\n",
    "### 3. Rectifier Function (Most used Function)\n",
    "### $$ \\phi(X) = max(X,0) $$\n",
    "\n",
    "### 4. Hyperbolic Tangent Function (tanh)\n",
    "### $$ \\phi(X) = \\frac{1-e^{-2X}}{1+e^{-2X}} $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# How do Neural Networks work?\n",
    "\n",
    "Ex. For predicting the Property Prices in an Area.\n",
    "\n",
    "<img src=\"ANN Working.png\" width=700>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# How do NNs Learn?\n",
    "\n",
    "A single Neuron is called **Perceptron**, and it learns by adjusting the weights ot reduce the Cost Function.\n",
    "\n",
    "<img src=\"ANN Learning.png\" width=750>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gradient Descent\n",
    "\n",
    "## 1. Brute Force Approach\n",
    "We just try for 1000 different values of the **Weights** and we find the weights which gives us the less Cost Function.\n",
    "<img src=\"Gradient Descent_Brute Force.png\" width=550>\n",
    "\n",
    "But When you have Hundreds of weights the **Curse of Dimensionality** comes into picture, And it is not going to work.\n",
    "\n",
    "## 2. Gradient Descent\n",
    "Hence we need the **Gradient Descent**.\n",
    "1. We start at a random point.\n",
    "2. we look for the slope at that point, which tells the direction of the ball to roll.\n",
    "3. and we keep on doing it untill we reach the minimum Cost Function.\n",
    "<img src=\"Gradient Descent Ball_Roll_Method.png\" width=550>\n",
    "\n",
    "The gradient Descent reduces the training time from Years to Hours, Bcz it can see the direction to reach the minima faster. \n",
    "\n",
    "But the Limitation is this method requires the Cost Function toi be **Convex**(just as shown in the above image). \n",
    "\n",
    "What if our Cost Func is not convex?\n",
    "- Then we will be stuck at a local minima just as shown in the Image below.\n",
    "\n",
    "<img src=\"local minima.png\" width=500>\n",
    "\n",
    "- The Solution is **Stochastic Gradient Descent**.\n",
    "\n",
    "## 3. Stochastic Gradient Descent\n",
    "* What's the Difference between Normal/Batch Gradient Descent and Stochastic Gradient Descent?\n",
    "    1. In Normal/Batch Gradient Descent:\n",
    "        - We feed all the rows into the Neural Network at once.\n",
    "        - then we Calculate the Cost Function\n",
    "        - then we Adjust the weights\n",
    "        \n",
    "    2. In Stochastic Gradient Descent:\n",
    "        - We feed a single row at a time.\n",
    "        - then we calculate the Cost Function\n",
    "        - and we adjust the weights emmediately.\n",
    "        - and we repeat the process for all the rows.\n",
    "        \n",
    "## 4. Mini batch Gradient Decsent \n",
    "* A combination of both batch gradient Descent and Stochastic Gradient Descent.\n",
    "\n",
    "Additional read: https://iamtrask.github.io/2015/07/27/python-network-part2/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Back Propagation:\n",
    "\n",
    "Back propagation is an advanced algorithm driven by very complex mathematics which allows us to adjust all the weights at the same time. \n",
    "\n",
    "Key Advantage - We basically know which part of the error each of our weights is responsible for.\n",
    "\n",
    "<img src=\"steps to build ANN.png\" width=850>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
