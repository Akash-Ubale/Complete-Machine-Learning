{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Boosting\n",
    "Boosting is an ensemble approach (meaning it involves several trees) that starts from a weaker decision maker and keeps on building the models such that the final prediction is the weighted sum of all the weaker decision-makers.\n",
    "\n",
    "The weights are assigned based on the performance of an individual tree.\n",
    "\n",
    "<img src= \"boosting_basic.png\" alt='boosting' style=\"width: 400px;\">\n",
    "\n",
    "\n",
    "Ensemble parameters are calculated in **stagewise way** which means that while calculating the subsequent weight, the learning from the previous tree is considered as well.\n",
    "\n",
    "In every machine learning model, the training objective is a sum of a loss function $L$ and regularisation $\\Omega$:\n",
    "\n",
    "$$\n",
    "obj = L + \\Omega\n",
    "$$\n",
    "\n",
    "The loss function controls the predictive power of an algorithm and the regularisation term controls its simplicity.\n",
    "\n",
    "There are several algorithms which use boosting. A few are discussed here.\n",
    "1. ADA Boost\n",
    "2. Gradient Boost\n",
    "3. XG Boost"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ada Boost (Adaptive Boosting)\n",
    "The steps to implement the Ada Boost algorithm using the decision trees are as follows:\n",
    "\n",
    "**Algorithm**:\n",
    "\n",
    "Assume that the number of training samples is denoted by $N$, and the number of iterations (created trees) is $M$. Notice that possible class outputs are $Y=\\{-1,1\\}$\n",
    "\n",
    "1. Initialize the observation weights  $w_i=\\frac{1}{N}$ where $i = 1,2, \\dots, N$ for all the samples.\n",
    "2. For $m=1$ to $M$:\n",
    "    - fit a classifier $G_m(x)$ to the training data using weights $w_i$,\n",
    "    - compute $err_m = \\frac{\\sum_{i=1}^{N} w_i I (y_i \\neq G_m(x))}{\\sum_{i=1}^{N}w_i}$,\n",
    "    - compute $\\alpha_m = \\frac {1}{2} \\log (\\frac{(1-err_m)}{err_m})$. This is the contribution of that tree to the final result.\n",
    "    - calculate the new weights using the formula:\n",
    "    \n",
    "    $w_i \\leftarrow w_i \\cdot \\exp [\\alpha_m \\cdot I (y_i \\neq G_m(x)]$, where $i = 1,2, \\dots, N$\n",
    "- Normalize the new sample  weights so that their sum is 1.\n",
    "- Construct the next tree using the new weights\n",
    "\n",
    "\n",
    " 3. At the end, compare the summation of results from all the trees and the final result is either the one with the highest sum(for regression) or it is the class which has the most weighted voted average(for classification).\n",
    "\n",
    "       Output $G_m(x) = argmax [\\sum_{m=1}^{M} \\alpha_m G_m(x)]$ (Regression)\n",
    "\n",
    "       Output $G_m(x) = sigm [\\sum_{m=1}^{M} \\alpha_m G_m(x)]$ (Classification)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
