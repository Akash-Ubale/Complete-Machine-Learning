{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Importing libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import all the necessary files here or else you can import them when you need.\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Importing Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dataset:\n",
      "    Country   Age   Salary Purchased\n",
      "0   France  44.0  72000.0        No\n",
      "1    Spain  27.0  48000.0       Yes\n",
      "2  Germany  30.0  54000.0        No\n",
      "3    Spain  38.0  61000.0        No\n",
      "4  Germany  40.0      NaN       Yes\n",
      "\n",
      "x:\n",
      " [['France' 44.0 72000.0]\n",
      " ['Spain' 27.0 48000.0]\n",
      " ['Germany' 30.0 54000.0]\n",
      " ['Spain' 38.0 61000.0]\n",
      " ['Germany' 40.0 nan]\n",
      " ['France' 35.0 58000.0]\n",
      " ['Spain' nan 52000.0]\n",
      " ['France' 48.0 79000.0]\n",
      " ['Germany' 50.0 83000.0]\n",
      " ['France' 37.0 67000.0]]\n",
      "\n",
      "y:\n",
      " ['No' 'Yes' 'No' 'No' 'Yes' 'Yes' 'No' 'Yes' 'No' 'Yes']\n"
     ]
    }
   ],
   "source": [
    "dataset = pd.read_csv(\"Data.csv\")\n",
    "print(\"dataset:\\n\",dataset.head())\n",
    "\n",
    "# Now we need to separate the Features and label bcz our ML Models expects the data to be this way.\n",
    "x = dataset.iloc[:, :-1].values # iloc - Locate Indices\n",
    "y = dataset.iloc[:, -1].values\n",
    "print(\"\\nx:\\n\", x)\n",
    "print(\"\\ny:\\n\", y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Taking care of missing values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([['France', 44.0, 72000.0],\n",
       "       ['Spain', 27.0, 48000.0],\n",
       "       ['Germany', 30.0, 54000.0],\n",
       "       ['Spain', 38.0, 61000.0],\n",
       "       ['Germany', 40.0, 63777.77777777778],\n",
       "       ['France', 35.0, 58000.0],\n",
       "       ['Spain', 38.77777777777778, 52000.0],\n",
       "       ['France', 48.0, 79000.0],\n",
       "       ['Germany', 50.0, 83000.0],\n",
       "       ['France', 37.0, 67000.0]], dtype=object)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.impute import SimpleImputer\n",
    "imputer = SimpleImputer(missing_values=np.nan, strategy=\"mean\")\n",
    "imputer.fit(x[:, 1:3])\n",
    "x[: ,[1,2]]  = imputer.transform(x[:, 1:3])\n",
    "x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Encoding Categorical Data\n",
    "\n",
    "Why do we need to encode categorical data? \n",
    "- ML Models works only on Numerical Data. \n",
    "\n",
    "The methods to encode categorical features\n",
    "1. One-Hot Enccoding \n",
    "2. Label Encoding      \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4.1. One-Hot Encoding\n",
    "If the Feature column has N categories , One-hot encoding technique creates N columns. i.e. It creates binary vectors for each category."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.0 0.0 44.0 72000.0]\n",
      " [0.0 1.0 27.0 48000.0]\n",
      " [1.0 0.0 30.0 54000.0]\n",
      " [0.0 1.0 38.0 61000.0]\n",
      " [1.0 0.0 40.0 63777.77777777778]\n",
      " [0.0 0.0 35.0 58000.0]\n",
      " [0.0 1.0 38.77777777777778 52000.0]\n",
      " [0.0 0.0 48.0 79000.0]\n",
      " [1.0 0.0 50.0 83000.0]\n",
      " [0.0 0.0 37.0 67000.0]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "ct = ColumnTransformer(transformers=[('encoder',OneHotEncoder(drop=\"first\"),[0])], remainder=\"passthrough\")\n",
    "x = ct.fit_transform(x)\n",
    "\n",
    "# our ML Model expects training data to be in np.array formt\n",
    "x = np.array(x)\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.2. Label Encoding\n",
    "If we encode the classes as\n",
    "\n",
    "        France = 1\n",
    "        Germany = 2\n",
    "        Spain = 3\n",
    "The problem with this approach is that Our model could interprete that 2 > 1 i.e. Germany > France but that's not the case. Hence we have used One-hot Encoding for categorical features.\n",
    "\n",
    "label Encoding can be used where the data is Qualitative. Ex. Good, Better, Best; here we can do \n",
    "\n",
    "    Good = 0\n",
    "    Better = 1\n",
    "    Best = 2\n",
    "    \n",
    "To demonstrate Label encoding we can perform it on Dependent variable i.e. 'y'. since it has only 2 classes which will get encoded as 0 and 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 1, 0, 0, 1, 1, 0, 1, 0, 1])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "le = LabelEncoder()\n",
    "y = le.fit_transform(y)\n",
    "y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Splitting the Dataset into training and testing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.2, random_state=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.0 1.0 38.77777777777778 52000.0]\n",
      " [1.0 0.0 40.0 63777.77777777778]\n",
      " [0.0 0.0 44.0 72000.0]\n",
      " [0.0 1.0 38.0 61000.0]\n",
      " [0.0 1.0 27.0 48000.0]\n",
      " [0.0 0.0 48.0 79000.0]\n",
      " [1.0 0.0 50.0 83000.0]\n",
      " [0.0 0.0 35.0 58000.0]]\n"
     ]
    }
   ],
   "source": [
    "print(x_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1.0 0.0 30.0 54000.0]\n",
      " [0.0 0.0 37.0 67000.0]]\n"
     ]
    }
   ],
   "source": [
    "print(x_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6. Feature Scaling\n",
    "\n",
    "**What does it do?**\n",
    "- Feature scaling puts all our features on to the same scale.\n",
    "\n",
    "**Why do we need it?**\n",
    "- For some of the Models, in order to avoid some features to be dominated by other features in such a way that the dominated features are not even considered by the ML Model.\n",
    "\n",
    "**Do we have to use Feature Scaling with every model?**\n",
    "- No.\n",
    "- Feature scaling is essential for machine learning algorithms that calculate distances between data. If the data is not scaled, the feature with a higher value range starts dominating when calculating distances.\n",
    "- in linear regression we won't need the Scaling bcz each variable is multiplied by coeffient, and while learning the coeffieins these coeffients will compensate by taking small values for large variables.\n",
    "\n",
    "**When Sould we use feature Scaling? Before or after the train_test_split()?**\n",
    "- After Splitting the Dataset\n",
    "- Because, the testset is supposed to be the new data, so we are not supposed to work with it.\n",
    "- some information leakge may happen if we work with test data which may lead to over fitting.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Standardization:\n",
    "$$ X_{standard} = \\frac{X - Mean(X)}{Standard~Deviation(X)} $$\n",
    "* all the features will take values between -3 to +3.\n",
    "\n",
    "### Normalization\n",
    "$$ X_{Normal} = \\frac{X-Min(X)}{Max(X)-Min(X)} $$\n",
    "* all the features will take values between 0 to 1.\n",
    "* Normalization is used only when the features are normally distributed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.0 1.0 -0.19159184384578545 -1.0781259408412425]\n",
      " [1.0 0.0 -0.014117293757057777 -0.07013167641635372]\n",
      " [0.0 0.0 0.566708506533324 0.633562432710455]\n",
      " [0.0 1.0 -0.30453019390224867 -0.30786617274297867]\n",
      " [0.0 1.0 -1.9018011447007988 -1.420463615551582]\n",
      " [0.0 0.0 1.1475343068237058 1.232653363453549]\n",
      " [1.0 0.0 1.4379472069688968 1.5749910381638885]\n",
      " [0.0 0.0 -0.7401495441200351 -0.5646194287757332]]\n",
      "[[1.0 0.0 -1.4661817944830124 -0.9069571034860727]\n",
      " [0.0 0.0 -0.44973664397484414 0.2056403393225306]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "sc = StandardScaler()\n",
    "x_train[:, 2:] = sc.fit_transform(x_train[:, 2:])\n",
    "x_test[:, 2:] = sc.transform(x_test[:, 2:])\n",
    "print(x_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1.0 0.0 -1.4661817944830124 -0.9069571034860727]\n",
      " [0.0 0.0 -0.44973664397484414 0.2056403393225306]]\n"
     ]
    }
   ],
   "source": [
    "print(x_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now all our values are in the same scale.\n",
    "\n",
    "Congrats ðŸŽ‰ðŸŽŠ your data is ready for model training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
